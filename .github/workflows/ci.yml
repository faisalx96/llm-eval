name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly performance benchmarks
    - cron: '0 2 * * *'

env:
  PYTHONPATH: ${{ github.workspace }}
  # Test environment variables (use secrets for real values)
  LANGFUSE_PUBLIC_KEY: ${{ secrets.LANGFUSE_PUBLIC_KEY || 'pk-test-key' }}
  LANGFUSE_SECRET_KEY: ${{ secrets.LANGFUSE_SECRET_KEY || 'sk-test-key' }}
  LANGFUSE_HOST: ${{ secrets.LANGFUSE_HOST || 'https://test.langfuse.com' }}

jobs:
  lint-and-format:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort mypy flake8
        pip install -e .[dev]
    
    - name: Format check with Black
      run: black --check --diff llm_eval tests
    
    - name: Import sorting check with isort
      run: isort --check-only --diff llm_eval tests
    
    - name: Lint with flake8
      run: |
        flake8 llm_eval tests --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 llm_eval tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type checking with mypy
      run: mypy llm_eval --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Type checking is advisory for now

  test-matrix:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11']
        exclude:
          # Exclude some combinations to reduce job count
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/setup.py') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install pytest-xdist pytest-timeout pytest-cov
    
    - name: Run unit tests
      run: |
        pytest tests/unit -v \
          --cov=llm_eval \
          --cov-report=xml \
          --cov-report=term-missing \
          --timeout=300 \
          -n auto
    
    - name: Run integration tests
      run: |
        pytest tests/integration -v \
          --timeout=600 \
          -n auto
    
    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  test-optional-dependencies:
    name: Test Optional Dependencies
    runs-on: ubuntu-latest
    strategy:
      matrix:
        extras: ['deepeval', 'langchain', 'openai', 'all']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install with ${{ matrix.extras }} extras
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,${{ matrix.extras }}]
    
    - name: Test import capabilities
      run: |
        python -c "
        import llm_eval
        from llm_eval.metrics import registry
        print('Available metrics:', registry.list_metrics())
        "
    
    - name: Run subset of tests
      run: |
        pytest tests/unit/test_metrics.py -v \
          --timeout=300
      continue-on-error: ${{ matrix.extras == 'deepeval' }}  # DeepEval tests may need API keys

  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[benchmark]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install psutil
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance -v \
          --timeout=1800 \
          --tb=short
    
    - name: Upload benchmark results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          htmlcov/
          performance_report.json
        retention-days: 30

  export-validation:
    name: Export Format Validation
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install jsonschema pandas openpyxl
    
    - name: Test JSON export format
      run: |
        python -c "
        import json
        from llm_eval.core.results import EvaluationResult
        
        # Create test result
        result = EvaluationResult('test', 'test-run', ['metric1'])
        result.add_result('item1', {'scores': {'metric1': 0.8}, 'success': True})
        result.finish()
        
        # Test JSON export
        json_path = result.save_json('test_export.json')
        with open(json_path) as f:
            data = json.load(f)
        
        # Validate structure
        assert 'dataset_name' in data
        assert 'results' in data
        assert 'metric_stats' in data
        print('JSON export validation: PASSED')
        "
    
    - name: Test CSV export format
      run: |
        python -c "
        import csv
        from llm_eval.core.results import EvaluationResult
        
        # Create test result
        result = EvaluationResult('test', 'test-run', ['metric1'])
        result.add_result('item1', {'scores': {'metric1': 0.8}, 'success': True})
        result.finish()
        
        # Test CSV export
        csv_path = result.save_csv('test_export.csv')
        with open(csv_path, newline='') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        
        # Validate structure
        assert len(rows) == 1
        assert 'item_id' in rows[0]
        assert 'metric_metric1' in rows[0]
        print('CSV export validation: PASSED')
        "

  cli-integration:
    name: CLI Integration Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Create test task file
      run: |
        cat > test_task.py << 'EOF'
        def simple_echo(input_text):
            """Simple echo task for testing."""
            return f"Echo: {input_text}"
        EOF
    
    - name: Test CLI help
      run: |
        llm-eval --help
    
    - name: Test CLI parameter validation
      run: |
        # This should fail with proper error message
        llm-eval --task-file test_task.py --task-function simple_echo --dataset nonexistent --metrics exact_match || echo "Expected failure"

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        pip install -e .
    
    - name: Check for security vulnerabilities in dependencies
      run: safety check
      continue-on-error: true
    
    - name: Run bandit security linter
      run: bandit -r llm_eval -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: bandit-report.json
        retention-days: 30

  build-and-publish:
    name: Build and Publish
    runs-on: ubuntu-latest
    needs: [lint-and-format, test-matrix, export-validation, cli-integration]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for version calculation
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    
    - name: Build package
      run: python -m build
    
    - name: Check package
      run: twine check dist/*
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist-packages
        path: dist/
        retention-days: 90
    
    # Uncomment to publish to PyPI on tags
    # - name: Publish to PyPI
    #   if: startsWith(github.ref, 'refs/tags/')
    #   env:
    #     TWINE_USERNAME: __token__
    #     TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
    #   run: twine upload dist/*

  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [lint-and-format, test-matrix, export-validation]
    if: always()
    
    steps:
    - name: Check quality gates
      run: |
        # Check if critical jobs passed
        if [[ "${{ needs.lint-and-format.result }}" != "success" ]]; then
          echo "❌ Code quality checks failed"
          exit 1
        fi
        
        if [[ "${{ needs.test-matrix.result }}" != "success" ]]; then
          echo "❌ Test suite failed"
          exit 1
        fi
        
        if [[ "${{ needs.export-validation.result }}" != "success" ]]; then
          echo "❌ Export validation failed"
          exit 1
        fi
        
        echo "✅ All quality gates passed"
    
    - name: Post quality summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const { needs } = context.payload.workflow_run || { needs: {} };
          
          let summary = '## 🔍 Quality Gates Summary\n\n';
          summary += '| Check | Status |\n';
          summary += '|-------|--------|\n';
          summary += `| Code Quality | ${{ needs.lint-and-format.result == 'success' && '✅ Passed' || '❌ Failed' }} |\n`;
          summary += `| Test Suite | ${{ needs.test-matrix.result == 'success' && '✅ Passed' || '❌ Failed' }} |\n`;
          summary += `| Export Validation | ${{ needs.export-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |\n`;
          
          // Post comment on PR
          if (context.payload.pull_request) {
            github.rest.issues.createComment({
              issue_number: context.payload.pull_request.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }