{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Eval Quick Start\n",
    "\n",
    "This notebook walks you through evaluating your first LLM application with LLM-Eval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install llm-eval\n",
    "\n",
    "from llm_eval import Evaluator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=\"pk-lf-c542f0f6-77fb-4704-b114-006fa90f5c0c\",\n",
    "    secret_key=\"sk-lf-219a7622-1acd-4e3b-8fba-d31f4e0dfe87\",\n",
    "    host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Check\n",
    "\n",
    "Your Langfuse credentials are already configured in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse configuration:\n",
      "Public key: ✓\n",
      "Secret key: ✓\n",
      "Host: https://cloud.langfuse.com\n"
     ]
    }
   ],
   "source": [
    "# Verify environment variables are set\n",
    "import os\n",
    "print(\"Langfuse configuration:\")\n",
    "print(f\"Public key: {'✓' if os.getenv('LANGFUSE_PUBLIC_KEY') else '✗'}\")\n",
    "print(f\"Secret key: {'✓' if os.getenv('LANGFUSE_SECRET_KEY') else '✗'}\")\n",
    "print(f\"Host: {os.getenv('LANGFUSE_HOST', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Your AI Function\n",
    "\n",
    "Let's create a simple Q&A bot to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def simple_qa_bot(question: str) -> str:\n",
    "    \"\"\"A simple Q&A bot for demonstration.\"\"\"\n",
    "    question = question.lower()\n",
    "    \n",
    "    if \"capital of france\" in question:\n",
    "        return \"Paris\"\n",
    "    elif \"2+2\" in question or \"2 + 2\" in question:\n",
    "        return \"4\"\n",
    "    elif \"python\" in question:\n",
    "        return \"Python is a high-level programming language known for its simplicity and readability.\"\n",
    "    elif \"hello\" in question or \"hi\" in question:\n",
    "        return \"Hello! How can I help you today?\"\n",
    "    else:\n",
    "        return \"I'm not sure about that. Could you ask something else?\"\n",
    "\n",
    "# Test it\n",
    "print(simple_qa_bot(\"What is the capital of France?\"))\n",
    "print(simple_qa_bot(\"What is 2+2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset in Langfuse\n",
    "\n",
    "**Before running the evaluation, you need to:**\n",
    "\n",
    "1. Go to your Langfuse dashboard\n",
    "2. Navigate to Datasets → New Dataset\n",
    "3. Create a dataset named \"quickstart-demo\"\n",
    "4. Add some test items:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": \"What is the capital of France?\",\n",
    "  \"expected_output\": \"Paris\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": \"What is 2+2?\", \n",
    "  \"expected_output\": \"4\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": \"Tell me about Python\",\n",
    "  \"expected_output\": \"Python is a programming language\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Your First Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecdaa1ef5314aaebd432edc7dfd6175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluator\n",
    "evaluator = Evaluator(\n",
    "    task=simple_qa_bot,\n",
    "    dataset=\"quickstart-demo\",  # This must match your Langfuse dataset name\n",
    "    metrics=[\"exact_match\", \"contains\", \"fuzzy_match\"]\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "results = evaluator.run()\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────── Overview ────────╮\n",
       "│ <span style=\"font-weight: bold\">Dataset:</span> quickstart-demo │\n",
       "│ <span style=\"font-weight: bold\">Total Items:</span> 3           │\n",
       "│ <span style=\"font-weight: bold\">Success Rate:</span> 100.0%     │\n",
       "│ <span style=\"font-weight: bold\">Duration:</span> 0.7s           │\n",
       "╰──────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────── Overview ────────╮\n",
       "│ \u001b[1mDataset:\u001b[0m quickstart-demo │\n",
       "│ \u001b[1mTotal Items:\u001b[0m 3           │\n",
       "│ \u001b[1mSuccess Rate:\u001b[0m 100.0%     │\n",
       "│ \u001b[1mDuration:\u001b[0m 0.7s           │\n",
       "╰──────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Evaluation Results: eval-20250725-195342          </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric      </span>┃<span style=\"font-weight: bold\">  Mean </span>┃<span style=\"font-weight: bold\"> Std Dev </span>┃<span style=\"font-weight: bold\">   Min </span>┃<span style=\"font-weight: bold\">   Max </span>┃<span style=\"font-weight: bold\"> Success </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> exact_match </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.667 </span>│   0.577 │ 0.000 │ 1.000 │<span style=\"color: #808000; text-decoration-color: #808000\">  100.0% </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> contains    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.667 </span>│   0.577 │ 0.000 │ 1.000 │<span style=\"color: #808000; text-decoration-color: #808000\">  100.0% </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> fuzzy_match </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.000 </span>│   0.000 │ 0.000 │ 0.000 │<span style=\"color: #808000; text-decoration-color: #808000\">    0.0% </span>│\n",
       "└─────────────┴───────┴─────────┴───────┴───────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m         Evaluation Results: eval-20250725-195342          \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Mean\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd Dev\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSuccess\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mexact_match\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.667\u001b[0m\u001b[32m \u001b[0m│   0.577 │ 0.000 │ 1.000 │\u001b[33m \u001b[0m\u001b[33m 100.0%\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mcontains   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.667\u001b[0m\u001b[32m \u001b[0m│   0.577 │ 0.000 │ 1.000 │\u001b[33m \u001b[0m\u001b[33m 100.0%\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mfuzzy_match\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.000\u001b[0m\u001b[32m \u001b[0m│   0.000 │ 0.000 │ 0.000 │\u001b[33m \u001b[0m\u001b[33m   0.0%\u001b[0m\u001b[33m \u001b[0m│\n",
       "└─────────────┴───────┴─────────┴───────┴───────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print a beautiful summary\n",
    "results.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 66.7%\n",
      "Average Similarity: 0.0%\n",
      "\n",
      "Total test cases: 3\n",
      "Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Access specific metrics\n",
    "exact_match_stats = results.get_metric_stats(\"exact_match\")\n",
    "print(f\"Exact Match Accuracy: {exact_match_stats['mean']:.1%}\")\n",
    "\n",
    "fuzzy_match_stats = results.get_metric_stats(\"fuzzy_match\")\n",
    "print(f\"Average Similarity: {fuzzy_match_stats['mean']:.1%}\")\n",
    "\n",
    "print(f\"\\nTotal test cases: {results.total_items}\")\n",
    "print(f\"Success rate: {results.success_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Custom Metrics\n",
    "\n",
    "Let's create a custom metric that checks response length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e455b00f06a14215a902e655f79c5483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────── Overview ────────╮\n",
       "│ <span style=\"font-weight: bold\">Dataset:</span> quickstart-demo │\n",
       "│ <span style=\"font-weight: bold\">Total Items:</span> 3           │\n",
       "│ <span style=\"font-weight: bold\">Success Rate:</span> 100.0%     │\n",
       "│ <span style=\"font-weight: bold\">Duration:</span> 0.6s           │\n",
       "╰──────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────── Overview ────────╮\n",
       "│ \u001b[1mDataset:\u001b[0m quickstart-demo │\n",
       "│ \u001b[1mTotal Items:\u001b[0m 3           │\n",
       "│ \u001b[1mSuccess Rate:\u001b[0m 100.0%     │\n",
       "│ \u001b[1mDuration:\u001b[0m 0.6s           │\n",
       "╰──────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Evaluation Results: eval-20250725-195343             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric             </span>┃<span style=\"font-weight: bold\">  Mean </span>┃<span style=\"font-weight: bold\"> Std Dev </span>┃<span style=\"font-weight: bold\">   Min </span>┃<span style=\"font-weight: bold\">   Max </span>┃<span style=\"font-weight: bold\"> Success </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> exact_match        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.667 </span>│   0.577 │ 0.000 │ 1.000 │<span style=\"color: #808000; text-decoration-color: #808000\">  100.0% </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> appropriate_length </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.667 </span>│   0.577 │ 0.000 │ 1.000 │<span style=\"color: #808000; text-decoration-color: #808000\">  100.0% </span>│\n",
       "└────────────────────┴───────┴─────────┴───────┴───────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             Evaluation Results: eval-20250725-195343             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Mean\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd Dev\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Min\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Max\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSuccess\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mexact_match       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.667\u001b[0m\u001b[32m \u001b[0m│   0.577 │ 0.000 │ 1.000 │\u001b[33m \u001b[0m\u001b[33m 100.0%\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mappropriate_length\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.667\u001b[0m\u001b[32m \u001b[0m│   0.577 │ 0.000 │ 1.000 │\u001b[33m \u001b[0m\u001b[33m 100.0%\u001b[0m\u001b[33m \u001b[0m│\n",
       "└────────────────────┴───────┴─────────┴───────┴───────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def appropriate_length(output: str, expected: str = None) -> float:\n",
    "    \"\"\"Check if response length is appropriate (not too short, not too long).\"\"\"\n",
    "    length = len(output)\n",
    "    \n",
    "    if length < 5:  # Too short\n",
    "        return 0.0\n",
    "    elif length > 200:  # Too long\n",
    "        return 0.5\n",
    "    else:  # Just right\n",
    "        return 1.0\n",
    "\n",
    "# Run evaluation with custom metric\n",
    "evaluator_custom = Evaluator(\n",
    "    task=simple_qa_bot,\n",
    "    dataset=\"quickstart-demo\",\n",
    "    metrics=[\"exact_match\", appropriate_length]  # Mix built-in and custom\n",
    ")\n",
    "\n",
    "results_custom = evaluator_custom.run()\n",
    "results_custom.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Advanced Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a414b6f025954afa91a91ea72a3adae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: qa-bot-experiment-4\n",
      "Duration: 0.8 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run with custom configuration\n",
    "evaluator_advanced = Evaluator(\n",
    "    task=simple_qa_bot,\n",
    "    dataset=\"quickstart-demo\",\n",
    "    metrics=[\"exact_match\", \"fuzzy_match\"],\n",
    "    config={\n",
    "        \"max_concurrency\": 3,  # Run 3 evaluations in parallel\n",
    "        \"timeout\": 5.0,        # 5 second timeout per test\n",
    "        \"run_name\": \"qa-bot-experiment-4\",\n",
    "        \"run_metadata\": {\n",
    "            \"version\": \"1.0\",\n",
    "            \"notes\": \"Testing basic Q&A functionality\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "results_advanced = evaluator_advanced.run()\n",
    "print(f\"Run name: {results_advanced.run_name}\")\n",
    "print(f\"Duration: {results_advanced.duration:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View Results in Langfuse\n",
    "\n",
    "Go to your Langfuse dashboard to see:\n",
    "- All evaluation traces\n",
    "- Detailed scoring for each test case\n",
    "- Performance metrics\n",
    "- Comparison between different runs\n",
    "\n",
    "Navigate to: Datasets → quickstart-demo → Experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Create more comprehensive datasets** with edge cases\n",
    "2. **Try different metrics** or create custom ones\n",
    "3. **Evaluate real LLM applications** (OpenAI, LangChain, etc.)\n",
    "4. **Set up automated evaluation** in your development workflow\n",
    "\n",
    "Check out more examples in the examples folder and read the User Guide for detailed instructions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
