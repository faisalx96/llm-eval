apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-eval-frontend
  namespace: llm-eval
  labels:
    app.kubernetes.io/name: llm-eval
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.3.0"
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: llm-eval-frontend
  template:
    metadata:
      labels:
        app: llm-eval-frontend
        app.kubernetes.io/name: llm-eval
        app.kubernetes.io/component: frontend
    spec:
      serviceAccountName: llm-eval-frontend
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      initContainers:
        - name: wait-for-api
          image: curlimages/curl:latest
          command:
            - sh
            - -c
            - |
              echo "Waiting for API to be ready..."
              until curl -f http://llm-eval-api:80/api/health; do
                echo "API is unavailable - sleeping"
                sleep 5
              done
              echo "API is ready!"
      containers:
        - name: frontend
          image: llm-eval-frontend:0.3.0
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP
          env:
            # Import environment from ConfigMaps
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: llm-eval-frontend-config
                  key: NODE_ENV
            - name: PORT
              valueFrom:
                configMapKeyRef:
                  name: llm-eval-frontend-config
                  key: PORT
            - name: HOSTNAME
              valueFrom:
                configMapKeyRef:
                  name: llm-eval-frontend-config
                  key: HOSTNAME
            - name: NEXT_PUBLIC_API_URL
              valueFrom:
                configMapKeyRef:
                  name: llm-eval-frontend-config
                  key: NEXT_PUBLIC_API_URL
            # Runtime configuration
            - name: NEXT_TELEMETRY_DISABLED
              value: "1"
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - llm-eval-frontend
                topologyKey: kubernetes.io/hostname
      tolerations:
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 30
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 30

---
apiVersion: v1
kind: Service
metadata:
  name: llm-eval-frontend
  namespace: llm-eval
  labels:
    app.kubernetes.io/name: llm-eval
    app.kubernetes.io/component: frontend
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app: llm-eval-frontend
  sessionAffinity: None

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llm-eval-frontend
  namespace: llm-eval
  labels:
    app.kubernetes.io/name: llm-eval
    app.kubernetes.io/component: frontend

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: llm-eval
  name: llm-eval-frontend-role
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: llm-eval-frontend-rolebinding
  namespace: llm-eval
subjects:
  - kind: ServiceAccount
    name: llm-eval-frontend
    namespace: llm-eval
roleRef:
  kind: Role
  name: llm-eval-frontend-role
  apiGroup: rbac.authorization.k8s.io